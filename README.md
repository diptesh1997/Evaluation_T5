
Word Overlap Based Evaluator: In the second approach, the evaluation process involves the utilization of word overlap evaluators. To facilitate this assessment, we employ a pretrained model known as "sent2vec." This model calculates the distance between each instance in the D-fake dataset and the 180 instances within the D-real dataset. Subsequently, the resulting distances are organized in an increasing order, effectively arranging the most similar sentences from the D-real dataset. These ten positions, denoting the relative ten nearest neighbors, are meticulously recorded for further analysis.

Following this preparatory step, a series of word overlap evaluators are utilized to comprehensively assess the generated questions. Each question in the dataset receives ten distinct scores from these evaluators, capturing various aspects of linguistic overlap and semantic correspondence. These individual scores are then aggregated, leading to the computation of key evaluation metrics such as BLEU, METEOR, and ROUGE.

BERT Score: In the third technique, the BERT model performs the STS (Semantic Textual Similarity) task wherein it compares the similarity between two texts using the cosine similarity measure. The model is trained on the STSB and SICK datasets. A cosine similarity of 1 indicates that the vectors are pointing in the same direction (perfect similarity), while a similarity of 0 indicates that the vectors are orthogonal (completely dissimilar).
